---
title: "Second analysis"
author: "Bhasheyam, Hemath"
date: "26 November 2017"
output: word_document
---
---
title: "Bi-cycle Analysis R code"
author: "Bhasheyam and Hemanth"
date: "18 October 2017"
output: word_document
 
---

# Read the Data

```{r}
datacy=read.csv("B:/MS/Fall-2017/ML/Project/Bycycle/Data/day.csv")
dim(datacy)
fix(datacy)

```



# Outliers are removed from the data
```{r}
boxplot(Filter(is.numeric,datacy))
```

# from the above we are able to see hum, windspeed, has some outliers


```{r}
boxplot(datacy$hum)
boxplot(datacy$windspeed)
```

```{r}
changed = datacy$windspeed[datacy$windspeed<0.38]
boxplot(changed)
```


#After Removing the Outliers:

```{r}
dim(datacy)
datacycle = datacy[datacy$windspeed < 0.37 & datacy$hum > 0.20,]
dim(datacycle)
```

1392 Instance are removed as they have outliers

```{r}
boxplot(Filter(is.numeric,datacycle))
boxplot(datacycle$windspeed)
```







# to find the better model and learning of the data 

Lets Introduced new column as Contclass

# analysis 1 - for the Count
    contclass = True -> High
    contclass = False -> Low
    
    
    
```{r}
datacycle$countclass = apply(datacycle, 1, function(x)  x[16] > mean(datacycle$cnt ))
set.seed(123)
jumble = runif(nrow(datacycle))
datacycle = datacycle[ordered(jumble),]
sampleindex = sample(2, nrow(datacycle),replace = TRUE, prob = c(0.80, 0.20))
Train = datacycle[sampleindex == 1,]
Test = datacycle[sampleindex == 2,]
dim(Train)
dim(Test)
```




```{r}
tdata = Train[3:14]
tdata = cbind(tdata,Train[17])

```


```{r}
library(OneR)
m =  optbin(tdata)
mod = OneR(m)
summary(mod)
```
```{r}
predictmod = predict(mod,Test)
eval_model(predictmod, Test)
```
### LINEAR REGRESSION
```{r}
scatter.smooth(x=Train$registered, y=Train$cnt, main="Causal ~ Count")
scatter.smooth(x=Train$casual, y=Train$cnt, main="Causal ~ Count")
```
```{r}
##Partioning the dataset to registered users & causal users
x_data = subset(datacycle, select = -c(cnt, countclass))
y_data = subset(datacycle, select = cnt)
x_data = subset(x_data, select = -registered)
x_data = subset(x_data, select = -casual)
```

```{r}
## Partioning 
#casual
ytrain_casual = Train['casual']
ytest_casual = Test['casual']

xtrain_casual = subset(Train, select = -c(casual, registered, cnt))
xtest_casual = subset(Test, select = -c( casual, countclass, cnt))



```

```{r}
x_train = subset(Train, select = -c(registered, cnt, instant, dteday, countclass))
x_test = subset(Test, select = -c(registered, cnt, instant, dteday, countclass))
```

```{r}
lmMod <- lm(x_train$casual~. ,  data=x_train)
summary(lmMod)
```

```{r}
casual_predict = predict(lmMod, x_test)
actuals_preds <- data.frame(cbind(actuals=x_test$casual, predicteds=casual_predict))
cor(actuals_preds) 
head(actuals_preds)
```
## 68.3% test accuracy for causal users 
```{r}
xtrain_reg = subset(Train, select = -c(casual, cnt, instant, dteday, countclass))
xtest_reg = subset(Test, select = -c(casual, cnt, instant, dteday, countclass))
```

```{r}
lmMod_reg <- lm(xtrain_reg$registered~. ,  data=xtrain_reg)
reg_predict = predict(lmMod_reg, xtest_reg)
actuals_preds_reg <- data.frame(cbind(actuals=xtest_reg$registered, predicteds=reg_predict))
cor(actuals_preds_reg)
```
## 55.7% test accuracy for registered users

#Transformed Linear Regreession
```{r}
Train_cpy = Train
Test_cpy = Test

```

#Convert the cnt to Log
```{r}
Train_cpy$log_cnt = log(Train$cnt)
Test_cpy$log_cnt = log(Test$cnt)
xtrain_log = subset(Train_cpy, select = -c(registered, casual, cnt, instant, dteday, countclass))
xtest_log = subset(Test_cpy, select = -c(casual, registered, cnt, instant, dteday, countclass))
```

```{r}
lmMod_log <- lm(xtrain_log$log_cnt~. ,  data=xtrain_log)
log_predict = predict(lmMod_log, xtest_log)
actuals_preds_log <- data.frame(cbind(actuals=xtest_log$log_cnt, predicteds=log_predict))
cor(actuals_preds_log)
```
## 68.91% accuracy 

# creating train and test task for the classifier analysis
# Drop features
```{r}
library(mlr)
traintaskf = makeClassifTask(data = Train  ,target = "countclass")
traintask  = makeClassifTask(data = Train  ,target = "countclass" , positive = "TRUE")
traintask = dropFeatures(task = traintaskf, features = c("dteday","instant","cnt","registered","casual"))
testtaskf = makeClassifTask(data = Test  ,target = "countclass")
testtask = dropFeatures(task = testtaskf, features = c("dteday","instant","cnt","registered","casual"))
```




# classifir -1 QDA - Quadratic Discriminant Analysis 
```{r}
cycleqda<- makeLearner("classif.qda", predict.type = "response")
cyclequdatrain = train(cycleqda, task = traintask)
qdapredict = predict(cyclequdatrain, testtask)
table(Test$countclass, qdapredict$data$response)


```


Here the Accuracy is 81 %

#classifier - 2 Logistic Regression
```{r}
cyclelr =  makeLearner("classif.logreg", predict.type = "response")
cyclequdatrain = train(cyclelr, task = traintask)
logrpredict = predict(cyclequdatrain, testtask)
table(Test$countclass, logrpredict$data$response)
```

Accuracy is 85%

#classifier 3 - Desicion tree

```{r}
cycletree =  makeLearner("classif.rpart", predict.type = "response")
 

treecv =  makeResampleDesc("CV",iters = 10L)


param = makeParamSet(
makeIntegerParam("minsplit",lower = 10, upper = 20),
makeIntegerParam("minbucket", lower = 5, upper = 10),
makeNumericParam("cp", lower = 0.001, upper = 0.1)
)


control = makeTuneControlGrid()

treetune <- tuneParams(learner = cycletree, resampling = treecv, task = traintask, par.set = param, control = control, measures = acc)

```



```{r}
treetune$x
tree = setHyperPars(cycletree, par.vals = treetune$x)
traintree = train(tree, traintask)
predicttree = predict(traintree, testtask)
table(Test$countclass,predicttree$data$response)
```

Here the Acuuracy is 88%

```{r}
plot(traintree$learner.model)
text(traintree$learner.model)
```
```{r}
library(rpart.plot)
prp(traintree$learner.model)
```
```{r}
library(rattle)	
fancyRpartPlot(traintree$learner.model)	
```


# random forest classfier

```{r}

randomforest = makeLearner("classif.randomForest",predict.type = "response", par.vals = list(ntree = 200, mtry = 3))
randomforest$par.vals = list(importance = TRUE)

randomparam <- makeParamSet(
makeIntegerParam("ntree",lower = 50, upper = 450),
makeIntegerParam("mtry", lower = 3, upper = 10),
makeIntegerParam("nodesize", lower = 10, upper = 40)
)
randomcontrol = makeTuneControlRandom(maxit = 30L)
randomcross = makeResampleDesc("CV",iter =  10L)
randomtune <- tuneParams(learner = randomforest, resampling = randomcross, task = traintask, par.set = randomparam, control = randomcontrol, measures = acc)

```


```{r}
randomtune$y
```


```{r}
randomtune$x
```

```{r}
randomtree = setHyperPars(randomforest, par.vals = randomtune$x)
randomtrain = train(randomtree, traintask)
getLearnerModel(randomtrain)

```


```{r}
randompredict = predict(randomtrain,testtask)
table(Test$countclass, randompredict$data$response)
```
91% Accuracy




```{r}
svmlearner = makeLearner("classif.ksvm", predict.type = "response")

randomcross = makeResampleDesc("CV",iters = 10L)

svmparameter<- makeParamSet(makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 2^x),
                            makeDiscreteParam("sigma", values = 2^c(-8,-4,0,4))) #RBF Kernel Parameter

svmcontrol = makeTuneControlGrid()
svmtune= tuneParams("classif.ksvm", task = traintask, resampling = randomcross, par.set  = svmparameter, control = svmcontrol, measures = acc)
```

```{r}
svmtune$y
```



```{r}
svmtune$x
```

```{r}
svmmodel = setHyperPars(svmlearner,par.vals = svmtune$x)
svmtrain = train(svmmodel, traintask)
getLearnerModel(svmtrain)
```
```{r}
predictsvm = predict(svmtrain, testtask)
table (Test$countclass , predictsvm$data$response)
```
0.88% accuracy


#bossting
```{r}
boost = makeLearner("classif.gbm", predict.type = "response")

gbmcontrol =  makeTuneControlRandom(maxit = 40L)

gbmcv = makeResampleDesc("CV",iters = 10L)






gbmparam =  makeParamSet(makeDiscreteParam("distribution", values = "bernoulli"),
makeIntegerParam("n.trees", lower = 500, upper = 1000), #number of trees
makeIntegerParam("interaction.depth", lower = 2, upper = 6), #depth of tree
makeIntegerParam("n.minobsinnode", lower = 10, upper = 50),
makeNumericParam("shrinkage",lower = 0.01, upper = 0.7))



gbmtune = tuneParams(learner = boost, task = traintask, par.set = gbmparam, control = gbmcontrol, measures = acc, resampling = gbmcv)
```



```{r}
gbmtune$y
```


```{r}
gbmtune$x
```


```{r}
gbmboost = setHyperPars(boost,par.vals = gbmtune$x)
```
```{r}
gbmtrain = train(gbmboost,traintask)
gbmpredict = predict(gbmtrain,testtask)
table(Test$countclass,gbmpredict$data$response)
```


89 % Accuracy

#lda

```{r}
ldalearner = makeLearner("classif.lda", predict.type = "response")
ldatrain = train(ldalearner, traintask)
predictlda = predict(ldatrain, testtask)
table(Test$countclass, predictlda$data$response)
```


the accuracy is 86%
#svm linear

```{r}
svmlearnerl = makeLearner("classif.ksvm", predict.type = "response")

randomcross = makeResampleDesc("CV",iters = 3L)

svmparameter<- makeParamSet(makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 2^x))


svmcontrol = makeTuneControlGrid()
svmtunel = tuneParams(svmlearnerl, task = traintask, resampling = randomcross, par.set  = svmparameter, control = svmcontrol, measures = acc)
```



```{r}
svmtune$x

```




```{r}
svmtunel$y
```



```{r}
svmmodell = setHyperPars(svmlearnerl,par.vals = svmtunel$x)
svmtrainl = train(svmmodell, traintask)
getLearnerModel(svmtrainl)

```


```{r}
predictsvml = predict(svmtrainl, testtask)
table (Test$countclass, predictsvml$data$response)
```
the accuracy is 88%
```{r}
dim(Train)
```

```{r}
library(class)

Train1 = Train[3:14]
Test1 = Test[3:14]
model=knn(train=Train1,test = Test1, cl=Train$countclass,k=15)
summary(model)
table(Test$countclass,model)
```



For k = 10 accuracy is 89%

for Higher the K value, the results are not better as the cluster stays very close to each other.


```{r}
names(Train)
```





